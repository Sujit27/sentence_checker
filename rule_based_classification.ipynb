{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of test set with Rule based open-source grammar checker LanguageTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install language-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_check\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the language for the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en-UK')\n",
    "# tool = language_check.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for grammar in a couple of custom correct and incorrect sentences. The first three are correct and the next three are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = [\"I see a man on the road.\",\n",
    "                    \"This sentence is correct.\",\n",
    "                    \"Are you alright ? \",\n",
    "                    \"I see an man on the road.\",  \n",
    "                    \"this sentence is not correct.\",\n",
    "                    \"Is you alright ?\"]\n",
    "sentence_checks = [len(tool.check(sample_sentence)) for sample_sentence in sample_sentences]\n",
    "print(sentence_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we see how language-check performs on some sample sentences. Returns 0 when there is no error in the sentence and non-zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the check for some sentences in the test set. The first four are correct and the next three are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "sample_sentences_test = [\"The testator died in 1945.\",\n",
    "                        \"I cannot write all things in the letter.\",\n",
    "                        \"The simple expedient of monitoring blood parameters was not undergone.\",\n",
    "                         \"No interest, however, will be paid for the amount so withheld.\",\n",
    "                        \"I want goes to the store.\",\n",
    "                        \"John was seen the book.\",\n",
    "                        \"John is tall on several occasions.\",\n",
    "                        \"That he can't five minutes without calling something \\\"quite sexy\\\" is quite beside used point.\"]\n",
    "sentence_checks_test = [len(tool.check(sample_sentence)) for sample_sentence in sample_sentences_test]\n",
    "print(sentence_checks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we see that language-check performed poorly in detecting some of the sample incorrect sentences. Next step : Check on whole test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test dataset files into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct sentences:  300\n",
      "Number of incorrect sentences:  300\n"
     ]
    }
   ],
   "source": [
    "correct_sentences_file = \"test_data/correct_sentence_set.txt\"\n",
    "incorrect_sentences_file = \"test_data/incorrect_sentence_set.txt\"\n",
    "\n",
    "with open(correct_sentences_file,'r') as f:\n",
    "    correct_sentences_file = f.readlines()\n",
    "    \n",
    "with open(incorrect_sentences_file,'r') as f:\n",
    "    incorrect_sentences_file = f.readlines()\n",
    "    \n",
    "print(\"Number of correct sentences: \", len(correct_sentences_file))\n",
    "print(\"Number of incorrect sentences: \", len(incorrect_sentences_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply check on each sentence in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sentences_classification = [len(tool.check(sentence)) for sentence in correct_sentences_file]\n",
    "incorrect_sentences_classification = [len(tool.check(sentence)) for sentence in incorrect_sentences_file]\n",
    "\n",
    "correct_sentences_classification = [0 if item == 0 else 1 for item in correct_sentences_classification]\n",
    "incorrect_sentences_classification = [0 if item == 0 else 1 for item in incorrect_sentences_classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[289  11]\n",
      " [232  68]]\n",
      "Accuracy:  0.595\n",
      "Recall:  0.22666666666666666\n",
      "Precision:  0.8607594936708861\n",
      "F1 score:  0.3588390501319261\n"
     ]
    }
   ],
   "source": [
    "true_labels = [0] * 300 + [1] * 300\n",
    "predicted_labels = correct_sentences_classification + incorrect_sentences_classification\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(true_labels,predicted_labels))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(true_labels,predicted_labels))\n",
    "print(\"Recall: \", metrics.recall_score(true_labels,predicted_labels))\n",
    "print(\"Precision: \", metrics.precision_score(true_labels,predicted_labels))\n",
    "print(\"F1 score: \", metrics.f1_score(true_labels,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we see that with the rule based technique we were not able to achieve a decent Precision and Recall on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
